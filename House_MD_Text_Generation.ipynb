{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House MD Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_50Soo03cJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3gIv2iB2SDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjP6w2hL3v0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/House MD Text Generation/Final Merged UTF8 2.txt','r',encoding = \"utf8\") as f:\n",
        "  text = f.read().lower().replace('\\n',' \\n ')\n",
        "print(text[1:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ucgLdGH72kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [w for w in text.split(' ') if w != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaJN38aHP4jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words[1:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ-d4O7w8w7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMzZqVYdBOwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_frequencies = {}\n",
        "#i = 1\n",
        "for word in words:\n",
        "    #print(i)\n",
        "    word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
        "    #i = i + 1\n",
        "#word_frequencies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7rxZuPDZJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_frequency = 10\n",
        "ignored_words = set()\n",
        "for k,v in word_frequencies.items():\n",
        "  if word_frequencies[k] < min_frequency:\n",
        "    ignored_words.add(k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnqhBcLxb882",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(ignored_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiHfFRzUShMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_values_from_list(the_list, ignored_words):\n",
        "   return [value for value in the_list if value not in ignored_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5cm5SReSjAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = remove_values_from_list(words,ignored_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVPhXrHPU58p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUSrWJduY4dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(set(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz2Lxyb5PV96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(words[1:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2F5y0FPck89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_edited = ' '.join(words)\n",
        "with open(\"/content/drive/My Drive/House MD Text Generation/House Edited Frequency.txt\",'w') as file:\n",
        "  file.write(text_edited)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91u2AjB9P89G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_indices = dict((word, index) for index, word in enumerate(set(words)))\n",
        "indices_word = dict((index, word) for index, word in enumerate(set(words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzhTpFtZQRh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words = []\n",
        "for word in words:\n",
        "    vectorized_words.append(word_indices[word])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZYfBSE5WmGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words = np.array(vectorized_words)\n",
        "#vectorized_ignored_words = np.array(vectorized_ignored_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WPB3UBZL45y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3LL6aUFL_qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJCKPF80L6dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words = np.reshape(vectorized_words,(-1,1))\n",
        "vectorized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJd8PdKvME4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_words.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CduUrFFMMID3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "vectorized_words = scaler.fit_transform(vectorized_words)\n",
        "vectorized_words'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpqHl-gKMPTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "seq_length = 500\n",
        "for i in range(seq_length,len(vectorized_words)):\n",
        "  X_train.append(vectorized_words[i-seq_length:i,0])\n",
        "  y_train.append(vectorized_words[i,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WdDVYQlMPju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train , y_train = np.array(X_train), np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HabYE0MUMPrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0jJnLmO2kCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.LSTM(128))\n",
        "model.add(keras.layers.Dense(len(set(words))))\n",
        "model.add(keras.layers.Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBv_wPSFN2cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, seq_length, len(set(words))), dtype=np.float16)\n",
        "        #print(x.shape)\n",
        "        y = np.zeros((batch_size,len(set(words))) , dtype=np.float16)\n",
        "        for i in range(batch_size):\n",
        "          for t, num in enumerate(sentence_list[index]):\n",
        "            x[i, t, num] = 1\n",
        "          y[i, next_word_list[i]] = 1\n",
        "\n",
        "          index = index + 1\n",
        "          if index == len(sentence_list):\n",
        "              index = 0\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jurqb-liVKCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQCpA9Dg6YbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSijN_JjOt4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV32zdyTzlOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(generator(X_train,y_train,batch_size),steps_per_epoch=int(len(X_train)/batch_size)+1,epochs = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFebqwAbtvOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdhXikWfXPB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_index = random.randint(0, len(text_edited) - seq_length - 1)\n",
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print(\"...Diversity:\", diversity)\n",
        "\n",
        "    generated = \"\"\n",
        "    sentence = text_edited[start_index : start_index + seq_length]\n",
        "    print('...Generating with seed: \"' + sentence + '\"')\n",
        "    gen_length = 1000\n",
        "    for i in range(gen_length):\n",
        "        x_pred = np.zeros((1, seq_length, len(set(words))))\n",
        "        for t, num in enumerate(sentence):\n",
        "            x_pred[0, t, num] = 1.0\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds, diversity)\n",
        "        next_word = indices_word[next_index]\n",
        "        sentence = sentence.split(\" \")[1:].append(next_word)\n",
        "        generated += next_word\n",
        "\n",
        "    print(\"...Generated: \", generated)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}